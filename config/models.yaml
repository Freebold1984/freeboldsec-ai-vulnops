# Model Configuration for Freeboldsec AI VulnOps Framework
# Configure your AI model providers and routing preferences

models:
  # OpenAI GPT Models
  gpt-4-turbo:
    provider: "openai"
    model_id: "gpt-4-turbo-preview"
    api_key: "${OPENAI_API_KEY}"
    max_tokens: 4000
    temperature: 0.1
    specialized_for: ["exploit", "report"]
    description: "Advanced reasoning for complex exploit development and professional reporting"
  
  gpt-3.5-turbo:
    provider: "openai"
    model_id: "gpt-3.5-turbo"
    api_key: "${OPENAI_API_KEY}"
    max_tokens: 2000
    temperature: 0.2
    specialized_for: ["triage"]
    description: "Fast and efficient vulnerability triage and classification"

  # Anthropic Claude Models
  claude-3-opus:
    provider: "anthropic"
    model_id: "claude-3-opus-20240229"
    api_key: "${ANTHROPIC_API_KEY}"
    max_tokens: 4000
    temperature: 0.1
    specialized_for: ["recon", "auth_audit"]
    description: "Superior analytical capabilities for reconnaissance and authentication analysis"
  
  claude-3-sonnet:
    provider: "anthropic"
    model_id: "claude-3-sonnet-20240229"
    api_key: "${ANTHROPIC_API_KEY}"
    max_tokens: 3000
    temperature: 0.15
    specialized_for: ["triage", "report"]
    description: "Balanced model for vulnerability triage and report generation"

  # DeepSeek Models (for specialized coding tasks)
  deepseek-coder:
    provider: "deepseek"
    model_id: "deepseek-coder-33b-instruct"
    api_key: "${DEEPSEEK_API_KEY}"
    api_base: "https://api.deepseek.com/v1"
    max_tokens: 8000
    temperature: 0.1
    specialized_for: ["exploit"]
    description: "Specialized in code generation and exploit development"

  # Mistral Models (for fast processing)
  mistral-large:
    provider: "mistral"
    model_id: "mistral-large-latest"
    api_key: "${MISTRAL_API_KEY}"
    max_tokens: 3000
    temperature: 0.1
    specialized_for: ["triage", "recon"]
    description: "Fast processing for high-volume triage and reconnaissance"

  # Together AI Models (for open-source alternatives)
  wizard-coder:
    provider: "together"
    model_id: "WizardLM/WizardCoder-Python-34B-V1.0"
    api_key: "${TOGETHER_API_KEY}"
    max_tokens: 4000
    temperature: 0.1
    specialized_for: ["exploit"]
    description: "Open-source coding specialist for exploit development"

# Model Selection Strategy
selection_strategy:
  # Primary model for each task type
  default_models:
    triage: "claude-3-sonnet"
    recon: "claude-3-opus"
    exploit: "deepseek-coder"
    report: "gpt-4-turbo"
    auth_audit: "claude-3-opus"
  
  # Fallback models if primary is unavailable
  fallback_models:
    triage: ["mistral-large", "gpt-3.5-turbo"]
    recon: ["mistral-large", "claude-3-sonnet"]
    exploit: ["wizard-coder", "gpt-4-turbo"]
    report: ["claude-3-sonnet", "gpt-3.5-turbo"]
    auth_audit: ["claude-3-sonnet", "gpt-4-turbo"]

# Performance and Cost Optimization
optimization:
  # Enable model caching to reduce API calls
  enable_caching: true
  cache_duration_hours: 24
  
  # Rate limiting to prevent API quota exhaustion
  rate_limits:
    openai: 60  # requests per minute
    anthropic: 50
    deepseek: 30
    mistral: 100
    together: 40
  
  # Cost management
  max_monthly_spend:
    openai: 500.00
    anthropic: 400.00
    deepseek: 200.00
    mistral: 300.00
    together: 150.00
  
  # Token usage optimization
  max_context_tokens:
    small_tasks: 2000
    medium_tasks: 4000
    large_tasks: 8000

# Model Routing Rules
routing_rules:
  # Route based on file type and content
  file_type_routing:
    "*.json":
      contains_burp_log: "triage"
      contains_swagger: "recon"
      contains_vulnerability: "exploit"
    
    "*.md":
      contains_report: "report"
      contains_findings: "report"
    
    "*.js":
      always: "recon"
    
    "*.xml":
      contains_soap: "recon"
      contains_wsdl: "recon"
  
  # Route based on content analysis
  content_routing:
    keywords:
      vulnerability_types: ["xss", "sqli", "idor", "ssrf", "csrf"]
      auth_keywords: ["jwt", "oauth", "saml", "authentication", "authorization"]
      recon_keywords: ["endpoint", "api", "swagger", "javascript", "subdomain"]
      exploit_keywords: ["payload", "poc", "exploit", "shell", "reverse"]
  
  # Load balancing for high-volume tasks
  load_balancing:
    enable: true
    max_concurrent_requests: 10
    queue_timeout_seconds: 300

# Quality Assurance
quality_controls:
  # Minimum confidence thresholds
  confidence_thresholds:
    triage: 0.8
    recon: 0.7
    exploit: 0.9
    report: 0.85
    auth_audit: 0.8
  
  # Cross-validation settings
  enable_cross_validation: true
  cross_validation_models: 2  # Use 2 models for critical findings
  consensus_threshold: 0.7
  
  # Output validation
  validate_outputs:
    json_structure: true
    required_fields: true
    exploit_safety: true  # Prevent harmful payloads
    report_completeness: true

# Logging and Monitoring
logging:
  log_level: "INFO"
  log_api_calls: true
  log_token_usage: true
  log_costs: true
  
  # Sensitive data handling
  redact_api_keys: true
  redact_sensitive_data: true
  retention_days: 90

# Integration Settings
integrations:
  # Burp Suite MCP integration
  burp_mcp:
    auto_import_logs: true
    real_time_analysis: false
    batch_processing: true
  
  # GitHub integration for report storage
  github:
    auto_create_issues: false
    repository: "security-findings"
    branch: "main"
  
  # Notification settings
  notifications:
    enable_alerts: true
    critical_findings_webhook: "${WEBHOOK_URL}"
    daily_summary_email: "${NOTIFICATION_EMAIL}"
